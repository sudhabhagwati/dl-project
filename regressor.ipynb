{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import arlpy.plot as aplt\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as style\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style.use('seaborn-poster')\n",
    "style.use('seaborn-whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed for random number generator\n",
    "np.random.seed(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data from the csv file into a dataframe\n",
    "dataset = pd.read_csv('training_data/exdfp_renin.csv')\n",
    "# print(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-process the dataset to keep only the active compounds\n",
    "d = []\n",
    "mdataset = dataset\n",
    "for i in range(dataset.shape[0]):\n",
    "    if dataset['IC50'][i] <= 50000:\n",
    "        continue\n",
    "    else:\n",
    "        d.append(i)\n",
    "d.append(0)\n",
    "mdataset = dataset.drop(dataset.index[d])\n",
    "print(mdataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the pIC50 values in the dataset\n",
    "plt.plot(-1 * np.log10(1e-9*np.asarray(mdataset['IC50'])), 'b--', linewidth=2)\n",
    "plt.xlabel('Training example number')\n",
    "plt.ylabel('pIC50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of features as input dimension\n",
    "input_dim = mdataset.shape[1]-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input matrix to neural network of size (no. of training data X no. of features)\n",
    "X = np.asarray(mdataset[mdataset.columns[1:(mdataset.shape[1]-1)]])\n",
    "\n",
    "# only needed when individual test example before shuffle are to be verified for pIC50 values\n",
    "# note the corresponding number in the dataframe or excel file\n",
    "Q9 = X[9,:]\n",
    "Q9 = Q9.reshape(Q9.shape[0],1)\n",
    "\n",
    "Q15 = X[15,:]\n",
    "Q15 = Q15.reshape(Q15.shape[0],1)\n",
    "\n",
    "Q20 = X[20,:]\n",
    "Q20 = Q20.reshape(Q20.shape[0],1)\n",
    "\n",
    "Q30 = X[30,:]\n",
    "Q30 = Q30.reshape(Q30.shape[0],1)\n",
    "\n",
    "Q50 = X[50,:]\n",
    "Q50 = Q50.reshape(Q50.shape[0],1)\n",
    "\n",
    "Q100 = X[100,:]\n",
    "Q100 = Q100.reshape(Q100.shape[0],1)\n",
    "\n",
    "Q200 = X[200,:]\n",
    "Q200 = Q200.reshape(Q200.shape[0],1)\n",
    "\n",
    "Q300 = X[300,:]\n",
    "Q300 = Q300.reshape(Q300.shape[0],1)\n",
    "\n",
    "Q500 = X[500,:]\n",
    "Q500 = Q500.reshape(Q500.shape[0],1)\n",
    "\n",
    "Q800 = X[800,:]\n",
    "Q800 = Q800.reshape(Q800.shape[0],1)\n",
    "\n",
    "Q900 = X[900,:]\n",
    "Q900 = Q900.reshape(Q900.shape[0],1)\n",
    "\n",
    "Q1000 = X[1000,:]\n",
    "Q1000 = Q1000.reshape(Q1000.shape[0],1)\n",
    "\n",
    "Q1200 = X[1200,:]\n",
    "Q1200 = Q1200.reshape(Q1200.shape[0],1)\n",
    "\n",
    "Q1300 = X[1300,:]\n",
    "Q1300 = Q1300.reshape(Q1300.shape[0],1)\n",
    "\n",
    "Q1400 = X[1400,:]\n",
    "Q1400 = Q1400.reshape(Q1400.shape[0],1)\n",
    "\n",
    "Q1500 = X[1500,:]\n",
    "Q1500 = Q1500.reshape(Q1500.shape[0],1)\n",
    "\n",
    "Q1600 = X[1600,:]\n",
    "Q1600 = Q1600.reshape(Q1600.shape[0],1)\n",
    "\n",
    "Q1700 = X[1700,:]\n",
    "Q1700 = Q1700.reshape(Q1700.shape[0],1)\n",
    "\n",
    "Q1800 = X[1800,:]\n",
    "Q1800 = Q1800.reshape(Q1800.shape[0],1)\n",
    "\n",
    "Q1900 = X[1900,:]\n",
    "Q1900 = Q1900.reshape(Q1900.shape[0],1)\n",
    "\n",
    "Q2000 = X[2000,:]\n",
    "Q2000 = Q2000.reshape(Q2000.shape[0],1)\n",
    "\n",
    "Q2100 = X[2100,:]\n",
    "Q2100 = Q2100.reshape(Q2100.shape[0],1)\n",
    "\n",
    "Q2200 = X[2200,:]\n",
    "Q2200 = Q2200.reshape(Q2200.shape[0],1)\n",
    "\n",
    "Q2300 = X[2300,:]\n",
    "Q2300 = Q2300.reshape(Q2300.shape[0],1)\n",
    "\n",
    "Q2400 = X[2400,:]\n",
    "Q2400 = Q2400.reshape(Q2400.shape[0],1)\n",
    "\n",
    "Q2500 = X[2500,:]\n",
    "Q2500 = Q2500.reshape(Q2500.shape[0],1)\n",
    "\n",
    "\n",
    "# true output pIC50 values \n",
    "Y = -1 * np.log10(1e-9*np.asarray(mdataset['IC50']))\n",
    "\n",
    "# shuffle and reshape the data\n",
    "Z = np.zeros((X.shape[0],X.shape[1]+1))\n",
    "Z[:,:-1] = X\n",
    "Z[:,-1] = Y\n",
    "Z = shuffle(Z)\n",
    "X = Z[:,:-1]\n",
    "Y = Z[:,-1]\n",
    "Y = Y.reshape(mdataset.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.25)\n",
    "\n",
    "# normalize and scale both training and test inputs\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train = sc.transform(X_train)\n",
    "sc.fit(X_test)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a new metric function for computing coefficient of determination (r-square)\n",
    "def coeff_determination(y_true, y_pred):\n",
    "    SS_res =  tf.keras.backend.sum(tf.keras.backend.square( y_true-y_pred )) \n",
    "    SS_tot = tf.keras.backend.sum(tf.keras.backend.square( y_true - tf.keras.backend.mean(y_true) ) ) \n",
    "    return ( 1 - SS_res/(SS_tot + tf.keras.backend.epsilon()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# code for hyperparameter tuning\n",
    "# NOTE: DO NOT run this code unless you want to tune the parameters\n",
    "trloss = []\n",
    "teloss = []\n",
    "trrsqaure = []\n",
    "tersqaure = []\n",
    "for i in range(10, 201, 1):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(2, input_dim=input_dim, kernel_initializer='normal', activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "    model.add(tf.keras.layers.Dense(8, activation='relu', kernel_initializer='normal', kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "    model.add(tf.keras.layers.Dense(1, kernel_initializer='normal', activation='linear'))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam', metrics=[coeff_determination])\n",
    "    model.fit(X_train, y_train, epochs=i)\n",
    "    score_train = model.evaluate(X_train, y_train)\n",
    "    score_test = model.evaluate(X_test, y_test)\n",
    "    trloss.append(score_train[0])\n",
    "    trrsqaure.append(score_train[1])\n",
    "    teloss.append(score_test[0])\n",
    "    tersqaure.append(score_test[1])\n",
    "\n",
    "x_axis = np.arange(10, 201, 1)\n",
    "aplt.plot(x_axis, trloss, color='red', marker='*', legend='Training loss', ylim=[-1,2], hold=True)\n",
    "aplt.plot(x_axis, trrsqaure, color='red', marker='o', legend='Training rsquare', hold=True)\n",
    "aplt.plot(x_axis, teloss, color='blue', marker='*', legend='Test loss', hold=True)\n",
    "aplt.plot(x_axis, tersqaure, color='blue', legend='Test rsquare', marker='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sequential neural network model\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Dense(1024, input_dim=input_dim, kernel_initializer='normal', activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "model.add(tf.keras.layers.Dense(200, activation='relu', kernel_initializer='normal', kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "model.add(tf.keras.layers.Dense(1, kernel_initializer='normal', activation='linear'))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=[coeff_determination])\n",
    "model.fit(X_train, y_train, epochs=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model on training data\n",
    "scores = model.evaluate(X_train, y_train)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "# evaluate the model on test data\n",
    "scores = model.evaluate(X_test, y_test)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate the model using test data \n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only needed when individual test example before shuffle are to be verified for pIC50 values\n",
    "y_pred_Q9 = model.predict(Q9.transpose())\n",
    "y_pred_Q15 = model.predict(Q15.transpose())\n",
    "y_pred_Q20 = model.predict(Q20.transpose())\n",
    "y_pred_Q30 = model.predict(Q30.transpose())\n",
    "y_pred_Q50 = model.predict(Q50.transpose())\n",
    "y_pred_Q100 = model.predict(Q100.transpose())\n",
    "y_pred_Q200 = model.predict(Q200.transpose())\n",
    "y_pred_Q300 = model.predict(Q300.transpose())\n",
    "y_pred_Q500 = model.predict(Q500.transpose())\n",
    "y_pred_Q800 = model.predict(Q800.transpose())\n",
    "y_pred_Q900 = model.predict(Q900.transpose())\n",
    "y_pred_Q1000 = model.predict(Q1000.transpose())\n",
    "y_pred_Q1200 = model.predict(Q1200.transpose())\n",
    "y_pred_Q1300 = model.predict(Q1300.transpose())\n",
    "y_pred_Q1400 = model.predict(Q1400.transpose())\n",
    "y_pred_Q1500 = model.predict(Q1500.transpose())\n",
    "y_pred_Q1600 = model.predict(Q1600.transpose())\n",
    "y_pred_Q1700 = model.predict(Q1700.transpose())\n",
    "y_pred_Q1800 = model.predict(Q1800.transpose())\n",
    "y_pred_Q1900 = model.predict(Q1900.transpose())\n",
    "y_pred_Q2000 = model.predict(Q2000.transpose())\n",
    "y_pred_Q2100 = model.predict(Q2100.transpose())\n",
    "y_pred_Q2200 = model.predict(Q2200.transpose())\n",
    "y_pred_Q2300 = model.predict(Q2300.transpose())\n",
    "y_pred_Q2400 = model.predict(Q2400.transpose())\n",
    "y_pred_Q2500 = model.predict(Q2500.transpose())\n",
    "# y_pred_Q9_ic50 = np.power(10, -y_pred_Q9.squeeze())*1e9\n",
    "# y_pred_Q15_ic50 = np.power(10, -y_pred_Q15.squeeze())*1e9\n",
    "# y_pred_Q20_ic50 = np.power(10, -y_pred_Q20.squeeze())*1e9\n",
    "# y_pred_Q30_ic50 = np.power(10, -y_pred_Q30.squeeze())*1e9\n",
    "# y_pred_Q50_ic50 = np.power(10, -y_pred_Q50.squeeze())*1e9\n",
    "# y_pred_Q100_ic50 = np.power(10, -y_pred_Q100.squeeze())*1e9\n",
    "# y_pred_Q200_ic50 = np.power(10, -y_pred_Q200.squeeze())*1e9\n",
    "# y_pred_Q300_ic50 = np.power(10, -y_pred_Q300.squeeze())*1e9\n",
    "# y_pred_Q500_ic50 = np.power(10, -y_pred_Q500.squeeze())*1e9\n",
    "# y_pred_Q800_ic50 = np.power(10, -y_pred_Q800.squeeze())*1e9\n",
    "# y_pred_Q900_ic50 = np.power(10, -y_pred_Q900.squeeze())*1e9\n",
    "# y_pred_Q1000_ic50 = np.power(10, -y_pred_Q1000.squeeze())*1e9\n",
    "# y_pred_Q1200_ic50 = np.power(10, -y_pred_Q1200.squeeze())*1e9\n",
    "# y_pred_Q1300_ic50 = np.power(10, -y_pred_Q1300.squeeze())*1e9\n",
    "# y_pred_Q1400_ic50 = np.power(10, -y_pred_Q1400.squeeze())*1e9\n",
    "# y_pred_Q1500_ic50 = np.power(10, -y_pred_Q1500.squeeze())*1e9\n",
    "# y_pred_Q1600_ic50 = np.power(10, -y_pred_Q1600.squeeze())*1e9\n",
    "# y_pred_Q1700_ic50 = np.power(10, -y_pred_Q1700.squeeze())*1e9\n",
    "# y_pred_Q1800_ic50 = np.power(10, -y_pred_Q1800.squeeze())*1e9\n",
    "# y_pred_Q1900_ic50 = np.power(10, -y_pred_Q1900.squeeze())*1e9\n",
    "# y_pred_Q2000_ic50 = np.power(10, -y_pred_Q2000.squeeze())*1e9\n",
    "# y_pred_Q2100_ic50 = np.power(10, -y_pred_Q2100.squeeze())*1e9\n",
    "# y_pred_Q2200_ic50 = np.power(10, -y_pred_Q2200.squeeze())*1e9\n",
    "# y_pred_Q2300_ic50 = np.power(10, -y_pred_Q2300.squeeze())*1e9\n",
    "# y_pred_Q2400_ic50 = np.power(10, -y_pred_Q2400.squeeze())*1e9\n",
    "# y_pred_Q2500_ic50 = np.power(10, -y_pred_Q2500.squeeze())*1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_Q9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_Q9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_Q15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_Q20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_Q30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_Q50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_Q100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_Q200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_Q300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_Q500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_Q800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_Q900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_Q1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_Q1200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_Q1300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_Q1400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_Q1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_Q1600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_Q1700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_Q1800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_Q1900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_Q2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_Q2100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_Q2200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_Q2300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_Q2400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_Q2500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# residual plot\n",
    "residues = y_test - y_pred\n",
    "plt.plot(np.squeeze(residues), 'b-o', linewidth=0.5, markersize=6)\n",
    "plt.xlabel('Test example number')\n",
    "plt.ylabel('Residue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model prediction vs truth\n",
    "plt.figure(1, figsize=(25,10))\n",
    "plt.plot(np.squeeze(y_pred), 'b--*', linewidth=0.8, markersize=5, label='Predicted pIC50')\n",
    "plt.plot(np.squeeze(y_test), 'r--o', linewidth=0.8, markersize=5, label='Experimental pIC50')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('Test example number')\n",
    "plt.ylabel('pIC50')\n",
    "# plt.savefig('fig-r1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model prediction vs truth (zoomed in)\n",
    "plt.figure(1, figsize=(25,10))\n",
    "plt.plot(np.squeeze(y_pred[:100]), 'b--*', linewidth=0.8, markersize=5, label='Predicted pIC50')\n",
    "plt.plot(np.squeeze(y_test[:100]), 'r--o', linewidth=0.8, markersize=5, label='Experimental pIC50')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('Test example number')\n",
    "plt.ylabel('pIC50')\n",
    "# plt.savefig('fig-r2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot to visulaize the model output vs true output for test\n",
    "plt.scatter(np.squeeze(y_pred), np.squeeze(y_test), alpha=0.8, c='blue', s=20)\n",
    "plt.plot([0,12], [0,12], c='red', linewidth=1)\n",
    "plt.xlabel('Experimental pIC50')\n",
    "plt.ylabel('Predicted pIC50')\n",
    "# plt.savefig('fig-r3.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate the model using training data \n",
    "y_pred = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot to visulaize the model output vs true output for training \n",
    "plt.scatter(np.squeeze(y_pred), np.squeeze(y_train), alpha=0.8, c='blue', s=20)\n",
    "plt.plot([0,12], [0,12], c='red', linewidth=1)\n",
    "plt.xlabel('Experimental pIC50')\n",
    "plt.ylabel('Predicted pIC50')\n",
    "# plt.savefig('fig-r4.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the unknown data set from csv file to dataframe\n",
    "udataset = pd.read_csv('library_to_screen/exdfp_maybridge.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# curate the unknown dataset to remove duplicates\n",
    "cudataset = udataset\n",
    "d =[]\n",
    "for i in range(udataset.shape[0]-1):\n",
    "    if udataset.loc[i]['Name']==udataset.loc[i+1]['Name']:\n",
    "        d.append(i+1)\n",
    "cudataset = udataset.drop(udataset.index[d])\n",
    "print(cudataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cudataset.to_csv('library_to_screen/exdfp_maybridge_wo_duplicates.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the name of compounds that were predicted to be active by DNN classifier\n",
    "hits = pd.read_csv('output_hits.csv')\n",
    "print(hits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataset with only the actives predicted by DNN classifier\n",
    "hitdataset = cudataset\n",
    "d = []\n",
    "for i in range(cudataset.shape[0]):\n",
    "    if cudataset.iloc[i]['Name'] in list(hits['Name']):\n",
    "        continue\n",
    "    else:\n",
    "        d.append(i)\n",
    "hitdataset = cudataset.drop(cudataset.index[d])\n",
    "print(hitdataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the pIC50 values using the regression neural network model\n",
    "uX = np.asarray(hitdataset[hitdataset.columns[1:(hitdataset.shape[1])]])\n",
    "uY = model.predict(uX)\n",
    "\n",
    "# sort the indexes of the predicted pIC50 values in descending order\n",
    "w = np.squeeze(uY)\n",
    "w = np.argsort(w)\n",
    "w = w[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the pIC50 values using the regression neural network model\n",
    "uX = np.asarray(hitdataset[hitdataset.columns[1:(hitdataset.shape[1])]])\n",
    "uY = model.predict(uX)\n",
    "\n",
    "# sort the predicted pIC50 values in descending order\n",
    "z = np.squeeze(uY)\n",
    "# z = np.sort(z)\n",
    "# z = z[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Names of top 500 molecules\n",
    "d = {}\n",
    "for i in range(500):\n",
    "    d[hitdataset.iloc[w[i]]['Name']] = z[w[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the list of top 500 names into a csv file\n",
    "with open(\"top_500_molecules.csv\", \"w\") as outfile:\n",
    "    for name, pic in d.items():\n",
    "        outfile.write(name)\n",
    "        outfile.write('\\t')\n",
    "        outfile.write(str(pic))\n",
    "        outfile.write(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "ml-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
